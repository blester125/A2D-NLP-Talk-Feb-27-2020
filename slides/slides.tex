\documentclass{beamer}

\usefonttheme[onlymath]{serif}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{array}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{minted}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
}
\usemintedstyle{manni}
\newminted{python}{fontsize=\footnotesize}

\usetheme{Pittsburgh}

\usepackage{pgfpages}
\setbeamertemplate{note page}{\pagecolor{yellow!5}\insertnote}
\setbeameroption{show notes on second screen=right}

\usepackage{tikz}
\usetikzlibrary{fit}
\tikzset{%
  highlight/.style={rectangle,rounded corners,fill=red!15,draw,
    fill opacity=0.5,thick,inner sep=0pt}
}
\newcommand{\tikzmark}[2]{\tikz[overlay,remember picture,
  baseline=(#1.base)] \node (#1) {#2};}
%
\newcommand{\Highlight}[1][submatrix]{%
    \tikz[overlay,remember picture]{
    \node[highlight,fit=(left.north west) (right.south east)] (#1) {};}
}

\title{Your Neural Network for NLP is Probably Wrong}
\subtitle{Why You Need to Mask More Than You Think}
\author{Brian Lester}
\institute{Interactions}
\date{February, 27, 2020}

\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
\newcommand{\softmax}{\mathop{\rm softmax}\nolimits}
\newcommand{\similar}{\mathop{\rm sim}\nolimits}

\begin{document}

\frame{\titlepage}
\begin{section}{Bio}

    \begin{frame}
        \frametitle{Me}
        \begin{itemize}
            \item Work on NLP at Interactions
            \item My specialization is in Deep Learning
            \item Maintain \href{https://github.com/dpressel/mead-baseline}{Mead-Baseline}
        \end{itemize}
        \note{
            \begin{itemize}
                \item Why you should listen to me
                \item We use NLP to ``facilitate customer care interactions'' aka build better chat bots
                \item We use DL for most models so I have a lot of experience
                \item I help maintain Mead-Baseline, our open-source Deep Learning modeling software
                \item I have batched some complex operations
                \begin{itemize}
                    \item Batched the CRF
                    \item Batched Beam Search
                \end{itemize}
                \item I have tracked down a lot of hidden batch instability problems
                \item I am trying to get more involved with research so if anyone needs help with some idea, especially if
                    the implementation is tricky, hit me up.
            \end{itemize}
        }
    \end{frame}

\end{section} % Bio

\begin{section}{Motivation}
    \begin{subsection}{What is batching?}
        \begin{frame}
            \frametitle{Binary Logistic Regression}
            \begin{align*}
                n &\coloneqq \text{number of features} \\
                c &\coloneqq \text{number of class} \coloneqq 1 \\
                f &\in \R ^ {n} \\
                w &\in \R ^ {n} \\
                s &= \sum_{i=0}^{n} f_i * w_i \\
            \end{align*}
        \note{
            \begin{itemize}
                \item First we are going to show some examples of how batching works starting with Binary LR
                \item In binary LR we have a vector representing features and a vector of the same size representing
                    weights
                \item (Normally in NLP we have sparse features where we don't have a actual vector of features and use
                    something like a hashmap instead but inside a neural network everything is dense so we will use
                    dense here)
                \item We take the sum of the features weighted by the weights. This is our logit
                \item Normally we then us a function like sigmoid to create a final probability but we don't need to
                    worry about that for now.
            \end{itemize}
        }
        \end{frame}

\begin{frame}[fragile]
    \frametitle{Binary Logistic Regression}
    \begin{pythoncode}
                        s = np.dot(f, w)
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item We are representing our feature and weights as numpy vectors
            \item we can use the dot product which is the sum of the products of elements in the vectors
        \end{itemize}
    }
\end{frame}

        \begin{frame}
            \frametitle{Binary Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    &\tikzmark{left}{1}\tikzmark{right} & 2 & 3 & 4 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                w &= \left[ \begin{array}{*6{c}}
                    &\tikzmark{left}{5}\tikzmark{right} & 6 & 7 & 8 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= 5
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see the mechanics of what is happening in our dot product
                    \item We multiply these terms together and add the result into the s
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Binary Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & 1 & \tikzmark{left}{2} \tikzmark{right}{} & 3 & 4 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                w &= \left[ \begin{array}{*6{c}}
                    & 5 & \tikzmark{left}{6} \tikzmark{right}{} & 7 & 8 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= 17
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see the mechanics of what is happening in our dot product
                    \item We multiply these terms together and add the result into the s
                    \item And it continues on like this.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Binary Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1} & 2 & 3 & \tikzmark{right}{4} &
                    \end{array}
                    \right] \\
                \Highlight[first]
                w &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{5} & 6 & 7 & \tikzmark{right}{8} &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= 70
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we highlight the elements in these arrays that interact with each other.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                n &\coloneqq \text{number of features} \\
                c &\coloneqq \text{number of class} \\
                f &\in \R ^ {n} \\
                W &\in \R ^ {n \text{ x } c} \\
                s &\in \R ^ {c} \\
                \forall_{j \in c}\hspace{2pt} s_j &= \sum_{i=0}^{n} f_i * W_{ij} \\
            \end{align*}
            \note{
                \begin{itemize}
                    \item We now extend to multi class where instead of deciding between 2 class (0 and 1) we will
                        decide between several.
                    \item We still have a feature vector
                    \item We now have multiple weight vectors one for each class
                    \item We pack our weight vectors into a single matrix of size n by c
                    \item Now our logits are a vector, we have a score for each class
                    \item for each class we do this same dot product between the feature vector and one of the weight
                        vectors
                    \item Once we have the score we normally turn it into probabilities with the softmax function but we
                        can skip this for now.
                \end{itemize}
            }

        \end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-class Logistic Regression}
    \begin{pythoncode}
                    s = []
                    for w in W.T:
                        s.append(np.dot(f, w))
                    s = np.array(s)
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item Here we see code that calculates it exactly how I described it before where we explicitly do the dot
                between the feature and each weight vector in turn
        \end{itemize}
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Multi-class Logistic Regression}
    \begin{pythoncode}
                    s = np.dot(f, W)
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item We can actually do this in a single dot product! Lets see why
        \end{itemize}
    }
\end{frame}

        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1}\tikzmark{right}{} & 2 & 3 & 4 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5}\tikzmark{right}{} & 9  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & 8 & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5} \tikzmark{right}{} & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can see our feature vector and see our new weight metric
                    \item We how the weight matrix is two weight vectors next to each other
                    \item Here we see the elements of the feature vector and weight matrix are combined
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & 1 & \tikzmark{left}{2} \tikzmark{right}{} & 3 & 4 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & 9  & \\
                    & \tikzmark{left}{6}\tikzmark{right}{} & 10 & \\
                    & 7 & 11 & \\
                    & 8 & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{17} \tikzmark{right}{} & &
                    \end{array}
                    \right] \\
                \Highlight[thrid]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can see our feature vector and see our new weight metric
                    \item We how the weight matrix is two weight vectors next to each other
                    \item Here we see the elements of the feature vector and weight matrix are combined
                    \item And it goes on like this...
                \end{itemize}
            }
        \end{frame}


        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1} & 2 & 3 & \tikzmark{right}{4} &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5} & 9  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & \tikzmark{right}{8} & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{70}\tikzmark{right}{} & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can see our feature vector and see our new weight metric
                    \item We how the weight matrix is two weight vectors next to each other
                    \item Here we see the elements of the feature vector and weight matrix are combined
                    \item And it goes on like this...
                    \item We see here that the value in this first spot of the logits was calculated between the feature
                        vector and the first column in the weight matrix
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1}\tikzmark{right}{} & 2 & 3 & 4 &
                    \end{array}
                    \right] \\
                    \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & \tikzmark{left}{9}\tikzmark{right}{}  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & 8 & 12 &
                    \end{array}
                    \right] \\
                    \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & 70 & \tikzmark{left}{9}\tikzmark{right}{} &
                    \end{array}
                    \right] \\
                    \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can see our feature vector and see our new weight metric
                    \item We how the weight matrix is two weight vectors next to each other
                    \item Here we see the elements of the feature vector and the second column weight matrix are combined
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & 1 & \tikzmark{left}{2}\tikzmark{right}{} & 3 & 4 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & 9 & \\
                    & 6 & \tikzmark{left}{10}\tikzmark{right}{}  & \\
                    & 7 & 11 & \\
                    & 8 & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & 70 & \tikzmark{left}{9}\tikzmark{right}{} &
                    \end{array}
                    \right] \\
                \Highlight[thrid]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can see our feature vector and see our new weight metric
                    \item We how the weight matrix is two weight vectors next to each other
                    \item Here we see the elements of the feature vector and the second column weight matrix are combined
                    \item And it goes on like this...
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Multi-class Logistic Regression}
            \begin{align*}
                f &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1} & 2 & 3 & \tikzmark{right}{4} &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & \tikzmark{left}{9}  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & 8 & \tikzmark{right}{12} &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & 70 & \tikzmark{left}{9}\tikzmark{right}{} &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can see our feature vector and see our new weight metric
                    \item We how the weight matrix is two weight vectors next to each other
                    \item Here we see the elements of the feature vector and the second column weight matrix are combined
                    \item And it goes on like this...
                    \item We see here that the value in the second spot of the logits was calculated between the feature
                        vector and the second column in the weight matrix
                    \item So we can see here that we can calculate the logits for multiple classes at once between there
                        is no interaction between the columns of the matrix in vector-matrix multiplication.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                n &\coloneqq \text{number of features} \\
                c &\coloneqq \text{number of class} \\
                b &\coloneqq \text{number of examples} \\
                F &\in \R ^ {b \text{ x } n} \\
                W &\in \R ^ {n \text{ x } c} \\
                S &\in \R ^ {b \text{ x } c} \\
                \forall_{k \in b} \hspace{2pt} \forall_{j \in c} \hspace{2pt} S_{kj} &= \sum_{i=0}^{n} F_{ki} * W_{ij} \\
            \end{align*}
            \note{
                \begin{itemize}
                    \item We just saw how you can calculate the scores for multiple classes of a single example in a
                        single vector-matrix multiply. Now we will see how you can actually do this for multiple
                        examples at once.
                    \item Here we see Batched Logistic Regression
                    \item Like before we still have a weight matrix of n by c
                    \item Now we have stacked multiple feature vectors into a feature matrix with b rows where each row
                        represents a different feature vector.
                    \item Now our scores are a matrix were each row is the scores for an example and each column is the
                        score for some class
                \end{itemize}
            }
        \end{frame}

\begin{frame}[fragile]
    \frametitle{Batched Multi-class Logistic Regression}
    \begin{pythoncode}
                S = []
                for f in F:
                    S.append(np.dot(f, W))
                S = np.stack(S)
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item Here we see a straight forward way to do this, where we loop over the features and calculate the
                scores for each example separately.
        \end{itemize}
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Batched Multi-class Logistic Regression}
    \begin{pythoncode}
                S = np.dot(F, W)
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item But we can actually do one better, we can calculate the whole thing, all classes for all examples in a
                single shot!
        \end{itemize}
    }
\end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                F &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1}\tikzmark{right}{} & 2 & 3 & 4 & \\
                    & 13 & 14 & 15 & 16 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5}\tikzmark{right}{} & 9  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & 8 & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5} \tikzmark{right}{} & & \\
                    & & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see our new feature matrix is two feature vectors stacked on top of each other
                    \item Our weight matrix is still a n by c matrix of weight vectors standing next to each other
                    \item We are now doing matrix matrix multiplication
                    \item Here we see the values for the first class of the first example are using the values in the first
                        row of the feature vector and the first column of the weight matrix
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                F &= \left[ \begin{array}{*6{c}}
                    & 1 & \tikzmark{left}{2}\tikzmark{right}{} & 3 & 4 & \\
                    & 13 & 14 & 15 & 16 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & 9 \\
                    & \tikzmark{left}{6}\tikzmark{right}{} & 10  & \\
                    & 7 & 11 & \\
                    & 8 & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5} \tikzmark{right}{} & & \\
                    & & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see our new feature matrix is two feature vectors stacked on top of each other
                    \item Our weight matrix is still a n by c matrix of weight vectors standing next to each other
                    \item We are now doing matrix matrix multiplication
                    \item Here we see the values for the first class of the first example are using the values in the first
                        row of the feature vector and the first column of the weight matrix
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                F &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1} & 2 & 3 & \tikzmark{right}{4} & \\
                    & 13 & 14 & 15 & 16 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5} & 9  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & \tikzmark{right}{8} & 12 &\\
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{70} \tikzmark{right}{} & & \\
                    & & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see that the first example first class was only calculated by looking at the first row
                        and first column, nothing else was used.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            Put two slides of the first row and second column here
        \end{frame}

        \begin{frame}
            Put two slides of the first row and second column here
        \end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                F &= \left[ \begin{array}{*6{c}}
                    & \tikzmark{left}{1} & 2 & 3 & \tikzmark{right}{4} & \\
                    & 13 & 14 & 15 & 16 &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & \tikzmark{left}{9} & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & 8 & \tikzmark{right}{12} &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & 70 & \tikzmark{left}{110} \tikzmark{right}{} & \\
                    & & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item and now we can see here that the first example, second class is only using the first row of
                        the feature matrix and the last column of the weight matrix
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            Put two slides of the second row and first column here
        \end{frame}

        \begin{frame}
            Put two slides of the second row and first column here
        \end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                F &= \left[ \begin{array}{*6{c}}
                    & 1 & 2 & 3 & 4 & \\
                    & \tikzmark{left}{13} & 14 & 15 & \tikzmark{right}{16} &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & \tikzmark{left}{5} & 9  & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & \tikzmark{right}{8} & 12 &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & 70 & 110 & \\
                    & \tikzmark{left}{382}\tikzmark{right}{} & &
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Similar to above the second example first class uses the second row of the features and the
                        first column of the weights
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            Put two slides of the second row and second column here
        \end{frame}

        \begin{frame}
            Put two slides of the second row and second column here
        \end{frame}

        \begin{frame}
            \frametitle{Batched Multi-class Logistic Regression}
            \begin{align*}
                F &= \left[ \begin{array}{*6{c}}
                    & 1 & 2 & 3 & 4 & \\
                    & \tikzmark{left}{13} & 14 & 15 & \tikzmark{right}{16} &
                    \end{array}
                    \right] \\
                \Highlight[first]
                W &= \left[ \begin{array}{*4{c}}
                    & 5 & \tikzmark{left}{9} & \\
                    & 6 & 10 & \\
                    & 7 & 11 & \\
                    & 8 & \tikzmark{right}{12} &
                    \end{array}
                    \right] \\
                \Highlight[second]
                s &= \left[ \begin{array}{*4{c}}
                    & 70 & 110 & \\
                    & 382 & \tikzmark{left}{614} \tikzmark{right}{}&
                    \end{array}
                    \right] \\
                \Highlight[third]
            \end{align*}
            \note{
                \begin{itemize}
                    \item And finally the last elements uses the two unused rows and columns.
                    \item So here we have seen that we can stack independent examples together just like we could stack
                        together independent weight vectors to calculate the scores for multiple examples at once.
                    \item This works because the rows and columns in matrix-matrix multiplication all act independently
                        of each other.
                    \item Totally unrelated data an be next to each other and they won't interact as it flows through
                        your network
                    \item Running multiple examples is what we mean by batching.
                \end{itemize}
            }
        \end{frame}


    \end{subsection} % What

    \begin{subsection}{Why Batching?}

        \begin{frame}
            \frametitle{Full Gradient Descent}

            \begin{columns}
                \begin{column}{0.5\textwidth}
                    Picture representing Full GD Here, maybe a long and strong arrow?
                \end{column}
                \begin{column}{0.5\textwidth}
                    Pros:
                    \begin{itemize}
                        \item This is the True gradient
                        \item You step is guaranteed to yield better performance on all examples
                    \end{itemize}
                    Cons:
                    \begin{itemize}
                        \item It is slow
                        \item You need to run each example before you update any parameters
                    \end{itemize}
                \end{column}
            \end{columns}

            \note{
                \begin{itemize}
                    \item The way we train these models is with something called gradient descent
                    \item This basically means run an example through, calculate how wrong the predicted answer is (the
                        loss), and use the gradient (the derivative) of the parameters with respect to this loss to
                        calculate how to update the parameters.
                    \item In some cases people do full gradient descent where they calculate the answers for each
                        example and get the gradient for everything
                    \item this is good because this is the true gradient, this is the best step you can take to do
                        better next time
                    \item This is slow because you have to weight to run the network on all your data before you update
                        any parameters
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Stochastic Gradient Descent}
            \begin{columns}
                \begin{column}{0.5\textwidth}
                    Pros:
                    \begin{itemize}
                        \item You get to update your parameters each step.
                        \item You can get better generalization because to the randomness.
                    \end{itemize}
                    Cons:
                    \begin{itemize}
                        \item Your gradient could be wrong.
                        \item The gradient for one example can cause a change that is detrimental to another example.
                    \end{itemize}
                \end{column}
                \begin{column}{0.5\textwidth}
                    Picture of SGD here maybe a loss surface and a bunch of small arrows in all directions
                \end{column}
            \end{columns}
            \note{
                \begin{itemize}
                    \item On the opposite end of the spectrum we have stochastic gradient descent
                    \item We calculate a gradient and update parameters after each example
                    \item Talk about pros and cons
                \end{itemize}
            }

        \end{frame}

        \begin{frame}
            \frametitle{Mini-Batched Gradient Descent}
            \begin{columns}
                \begin{column}{0.5\textwidth}
                    Pros:
                    \begin{itemize}
                        \item You get to update your parameters more often.
                        \item You can a better approximation of your gradient.
                    \end{itemize}
                \end{column}
                \begin{column}{0.5\textwidth}
                    Cons:
                    \begin{itemize}
                        \item Your need to run the examples in a batch which can be tricky.
                        \item Minibatch size is now a hyperparameter.
                    \end{itemize}
                \end{column}
            \end{columns}
            \note{
                \begin{itemize}
                    \item This is a happy middle ground between the two
                    \item We calculate the gradient for a small batch of examples and update
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Gradient Accumulation?}
            \begin{itemize}
                \item We want to have batches for training dynamics
                \item Why do I actually have to batch?
                \item Can't I just run each example individually and accumulate the gradients in memory and apply them
                    when I get enough?
            \end{itemize}
            \note{
                \begin{itemize}
                    \item Gradient Accumulation is used when your networks are really large and can't fit on a single
                        GPU
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Speed}
                Picture of a GPU
            \note{
                \begin{itemize}
                    \item We showed before that we can express batched operations as matrix matrix multiplcations
                    \item Lucky for use we are really go at doing these matmuls fast
                    \item libraries and devices like GPUs are designed to do these is a parallel and fast way
                    \item we can stand on the shoulders of gaming to train models really fast
                \end{itemize}
            }
        \end{frame}

    \end{subsection} % Why

\end{section} % Motivation

\begin{section}{Batching is hard in NLP}

    \begin{frame}
        \frametitle{Batching in NLP}

        A vector of one lengths and a vector of a different length and something that shows how they can't be a batch

        \note{
            \begin{itemize}
                \item We saw before that batching involves stacking multiple vectors of the same shape into a matrix
                \item In NLP we often have examples that have different lengths
                \begin{itemize}
                    \item Because sentences have different lengths
                    \item Because words have different number of characters in them
                \end{itemize}
                \item we can't just batch any given sentences
            \end{itemize}
        }
    \end{frame}

    \begin{frame}
        \frametitle{Introducing \textless PAD\textgreater}

        The same sentences as above but using pad to make them match

        \note{
            \begin{itemize}
                \item We use a new symbol call \textless PAD\textgreater .
                \item We add this to the end of shorter sentence so that all the ones in a batch have the sample length.
                \item Now that they are all the same length we can turn them into a matrix.
            \end{itemize}
        }
    \end{frame}

    \begin{frame}
        \frametitle{The lengths vector}

        The same sentences as above but also show a vector of legnths for them

        \note{
            \begin{itemize}
                \item Now that some of our examples are longer because if this $<PAD>$ we need to track how long the
                    actual data is
                \item We store this in a vector
                \item This will be a vector of the same length as items in the batch
                \item The value will be the number of non padded elements in that example
                \item You can actually have padding on every example so the length of your time dimension is longer that
                    the maximum value in the lengths vector but this is sloppy and we won't really consider that case.
            \end{itemize}
        }
    \end{frame}

\end{section} % Hard in NLP

\begin{section}{Batching can Introduce Errors}

    % Something is weird with the slides in the frame here
    \begin{subsection}{Mean Pooling}
        \begin{frame}
            \frametitle{Mean Pooling}
            \begin{itemize}
            \item[]<1->$
                \left[ \begin{array}{*6{c}}
                    1 & 10 & 8 & 17 & 13 & 17
                \end{array} \right] = \frac{66}{6} = 11.0
            $
            \item[]<1->
            \item[]<2->$
                \left[ \begin{array}{*4{c}}
                    22 & 24 & 9 & 13
                \end{array} \right] = \frac{68}{4} = 17.0
            $
            \item[]<3->
            \item[]<3->$
                \left[ \begin{array}{*6{c}}
                    5 & 4 & 8 & 9 & 10 & 34 \\
                    6 & 3 & 1 & 4 & 0 & 0
                \end{array} \right] =
                \left[ \begin{array}{c}
                    \frac{66}{6} \\
                    \frac{68}{6}
                \end{array} \right] =
                \left[ \begin{array}{c}
                    11.0 \\
                    11.\bar{3}
                \end{array} \right]
            $
            \end{itemize}
            \note{
                \begin{itemize}
                    \item How we will look at how padding can get us in trouble
                    \item In most of these examples there is actually another dimension of features but we are going to
                        ignore it for simplicity
                    \item This is mean pooling can be used to aggregate over some dimension, for example we can pool the
                        features across a sentence to get the representation for the whole sentence.
                    \item Here we see the mean pooling for a vector of length 6
                    \item<2-> Here we see the mean pooling for a length of 4 (shorter)
                    \item<3-> How we see that in order to batch we need to pad the shorter vector
                    \item<3-> Now our denominator is much larger so our results is smaller!
                    \item<3-> Before I said that batching allows us to combine unrelated examples and calculate them in
                        one short but this illustrates the problems with padding,
                    \item<3-> When you padding goes wrong your results for some example will bleed over and effect
                        others which breaks the independence assumption we had before
                \end{itemize}
            }
        \end{frame}

\begin{frame}[fragile]
    \frametitle{Mean Pooling}

    \begin{pythoncode}
            >>> x
            array([[ 1, 10,  8, 17, 13, 17],
                   [22, 24,  9, 13,  0,  0]])
            >>> np.mean(x, axis=1)
            array([11.        , 11.33333333])
            >>> lengths
            array([6, 4])
            >>> np.sum(x, axis=1) / lengths
            array([11., 17.])
    \end{pythoncode}

    \note{
        \begin{itemize}
            \item We can solve this by explicitly using the lengths vector we had before.
            \item We can manually do our sum and then divide by the actual length instead of the length of the padded
                vector
            \item This assumes that the padded value is zero. Soon we will see how to insure that.
            \item This sort of thing is important because if you don't do it then values will dependent on what else is
                in the batch.
            \item This problem is especially important in training vs production environments, when training it is
                almost always in a batch with things that are longer but in prod you are often a single example where
                you are the longest thing.
        \end{itemize}
    }

\end{frame}

    \end{subsection} % Mean Pooling

    \begin{subsection}{Token Level Losses}

        \begin{frame}
            \frametitle{(Sparse) Cross Entropy Loss}
            \begin{align*}
                T &\coloneqq \text{length of example} \\
                V &\coloneqq \text{number of possible labels} \\
                s &\in \R^{T \text{ x } V} \\
                l &\in \N^T \\
                J &= - \sum_{i}^{T} s_i[l_i] \\
            \end{align*}

            \note{
                \begin{itemize}
                    \item The next example is in the idea of a token level loss. This is a loss where a loss is computed
                        at each step in a vector.
                    \item Here we see the cross entropy loss where we have a matrix of scores. Each row represents the
                        scores of a token and each column is the score for some tag
                    \item This is sparse cross entropy where each token only has one label so we can select out the
                        score for the right class instead of scaling the score with the value on the label
                    \item Our loss is then the score sum of the score for each token,
                    \item This kind of loss is often used for language modeling where the label is the index of the next
                        word or else sequence tagging where the label is the tokens tag.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{(Sparse) Cross Entropy Loss}
            \begin{align*}
                s &= \left[ \begin{array}{*5{c}}
                    -0.34 & 0.1 & -0.11 & \cdots & 0.001 \\
                    0.93 & -8.88 & -0.39 & \cdots & 0.12 \\
                    & & \vdots & & \\
                    -0.45 & 0.23 & 1.1 & \cdots & -0.3
                \end{array} \right] \\
                l &= \left[ \begin{array}{*5{c}}
                    2 & 1 & 10 & 5 & 1
                \end{array} \right] \\
                J &= \left[ \begin{array}{*5{c}}
                    -0.11 & -8.88 & 0.28 & 0.43 & 0.23
                \end{array} \right]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see some examples of score, labels, and the selected scores.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{(Sparse) Cross Entropy Loss}
            \begin{align*}
                s &= \left[ \begin{array}{*5{c}}
                    -0.34 & 0.1 & -0.11 & \cdots & 0.001 \\
                    0.93 & -8.88 & -0.39 & \cdots & 0.12 \\
                    & & \vdots & & \\
                    -0.45 & 0.23 & 1.1 & \cdots & -0.3
                \end{array} \right] \\
                l &= \left[ \begin{array}{*5{c}}
                    2 & 1 & 10 & 5 & 1 \\
                    5 & 6 & 3 & &
                \end{array} \right] \\
                J &= \left[ \begin{array}{*5{c}}
                    -0.11 & -8.88 & 0.28 & 0.43 & 0.23 \\
                    -3.4 & 0.6 & 0.37 & -956.0 & 10000.0
                \end{array} \right]
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we see what can happen when you are batching the shorter sentence. When you are
                        selecting the scores for the padding tokens they really could be anything
                    \item We don't really want the network to have to learn to output a padding symbol after the normal
                        length, that isn't a situation you would really have
                    \item We also don't want random lucky values to make our loss really small
                    \item We need a way to zero our the effects of these padded locations
                \end{itemize}
            }
        \end{frame}

\begin{frame}[fragile]
    \frametitle{Making a Mask}
    \begin{pythoncode}
        >>> lengths = np.array([5, 3, 4])
        >>> steps = np.arange(np.max(lengths))
        >>> steps
        array([0, 1, 2, 3, 4])
        >>> mask = (np.reshape(lengths, (-1, 1)) > steps).astype(np.int32)
        >>> mask
        array([[1, 1, 1, 1, 1],
               [1, 1, 0, 0, 0],
               [1, 1, 1, 1, 0]], dtype=int32)
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item Here we see how to create a mask. This mask has ones where the valid tokens are and a zero where the
                padding tokens are.
            \item We create this with broadcasting.
            \item We create a vector of incrementing steps along the sentence dimension and compare them to the lengths.
            \item When the length is longer than the current steps we are still valid so the value is one.
        \end{itemize}
    }
\end{frame}

        \begin{frame}
            \frametitle{(Sparse) Cross Entropy Loss}
            \begin{align*}
                J &= \left[ \begin{array}{*5{c}}
                    -0.11 & -8.88 & 0.28 & 0.43 & 0.23 \\
                    -3.4 & 0.6 & 0.37 & -956.0 & 10000.0
                \end{array} \right]\\
                \text{mask} &= \left[ \begin{array}{*5{c}}
                    1 & 1 & 1 & 1 & 1 \\
                    1 & 1 & 1 & 0 & 0
                \end{array} \right] \\
                J * \text{mask} &= \left[ \begin{array}{*5{c}}
                    -0.11 & -8.88 & 0.28 & 0.43 & 0.23 \\
                    -3.4 & 0.6 & 0.37 & 0 & 0
                \end{array} \right]\\
            \end{align*}
            \note{
                \begin{itemize}
                    \item Here we can use our mask to force the padding locations to zero.
                    \item We can then sum the values (or mean them with the lengths if you want).
                    \item Some toolkits like Pytorch can handle this for you, you give a value to the
                        \texttt{padding\_idx} parameter and it will ignore values where the label is that padding index
                \end{itemize}
            }
        \end{frame}

    \end{subsection} % Token Level Losses

    \begin{subsection}{Attention}

        \begin{frame}
            \frametitle{Attention}

            Picture of seq2seq attention and transformer self attention here

            \note{
                \begin{itemize}
                    \item Attention is used to contextualize a vector relative to others.
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Attention}

            \begin{align*}
                q &= \begin{bmatrix}
                        \vert \\
                        q_1\\
                        \vert
                    \end{bmatrix} \\
                V &= \begin{bmatrix}
                         \vert & \vert & & \vert \\
                         v_1 & v_2 & \cdots & v_n \\
                         \vert & \vert & & \vert
                     \end{bmatrix} \\
                s &= \similar(q, V) = \begin{bmatrix}
                        s_1 & s_2 & \cdots & s_n \\
                    \end{bmatrix}
            \end{align*}

            \note{
                \begin{itemize}
                    \item First we calculate a score between the vectors
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Attention}
            \begin{align*}
                s &= \similar(q, V) = \begin{bmatrix}
                        0.21 & -0.34 & \cdots & 0.55 \\
                    \end{bmatrix} \\
                \softmax(x) &= \frac{e^{x_i}}{\sum_j e^{x_j}} \\
                \alpha &= \softmax(s) = \begin{bmatrix}
                        0.1684 & 0.0977 & \cdots & 0.2379 \\
                    \end{bmatrix} \\
            \end{align*}
            \note{
                \begin{itemize}
                    \item Then we use the softmax so they sum to one and we can do a weighted average
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Attention}
            \begin{align*}
                V &= \begin{bmatrix}
                         \vert & \vert & & \vert \\
                         v_1 & v_2 & \cdots & v_n \\
                         \vert & \vert & & \vert
                     \end{bmatrix} \\
                s &= \similar(q, V) = \begin{bmatrix}
                        s_1 & s_2 & \cdots & s_n \\
                    \end{bmatrix} \\
                \alpha &= \softmax(s) \\
                v &= V * \alpha^{T} \\
            \end{align*}
            \note{
                \begin{itemize}
                    \item Then we do a weighted average of the vectors based on these calculated scores
                    \item Now $v$ is a combination of the vectors in $V$ weighted by their attention scores which were
                        calculated based on their similarity to q
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Attention}

            \begin{align*}
                V &= \begin{bmatrix}
                        \vert & \vert & & \vert & \vert \\
                        v_1 & v_2 & \cdots & \text{\textless PAD\textgreater} & \text{\textless PAD \textgreater} \\
                        \vert & \vert & & \vert & \vert \\
                     \end{bmatrix} \\
                s &= \begin{bmatrix}
                        s_1 & s_2 & \cdots & 0 & 0
                     \end{bmatrix}
            \end{align*}

            \note{
                \begin{itemize}
                    \item But what happens when we calculate the similarity between vectors that have padding on them?
                    \item With most similarity metrics (dot/scaled dot product) we get zeros for the similarity between
                        a pad and a vector (assuming the pad is zeros)
                    \item Is this enough to calculate the right thing?
                    \item Would I have brought this up if it was?
                \end{itemize}
            }
        \end{frame}

\begin{frame}[fragile]
    \frametitle{Attention}
    \begin{pythoncode}
  >>> x = np.array([
          [0.32, 0.11, 0.83, 0.9, 0.4],
          [0.5, 0.2, -1.0, 0, 0]
      ])
  >>> x
  array([[ 0.32,  0.11,  0.83,  0.9 ,  0.4 ],
         [ 0.5 ,  0.2 , -1.  ,  0.  ,  0.  ]])
  >>> softmax(x)
  array([[0.15759942, 0.12774761, 0.26244893, 0.28147862, 0.17072542],
         [0.31476139, 0.23318098, 0.07023276, 0.19091244, 0.19091244]])
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item Here we can see that when the scores going into the softmax are zero we will get non-zero scores out!
            \item This means that when we do the mixing with the weighted average we will 
            \item What we need to have there be zeros \textbf{After} the softmax which will zero out the contribution of
                the padding
        \end{itemize}
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Attention}
    \begin{pythoncode}
  >>> mask = make_mask(np.array([5, 3]))
  >>> mask
  array([[1, 1, 1, 1, 1],
         [1, 1, 1, 0, 0]], dtype=uint8)
  >>> attn = softmax(x) * mask
  >>> attn
  array([[0.15759942, 0.12774761, 0.26244893, 0.28147862, 0.17072542],
         [0.31476139, 0.23318098, 0.07023276, 0.        , 0.        ]])
  >>> np.sum(attn, axis=1)
  array([1.        , 0.61817513])
  >>> softmax(x[1:np.newaxis, 0:3])
  >>> array([[0.50917835, 0.3772086 , 0.11361305]])
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item Ok, lets just mask out those padding weights, now their values are zero so they won't contribute to
                the weighted average anymore.
            \item But we see that the values don't sum to one any more!
            \item This solution isn't enough
            \item We all see that the values calculated for the sorter batch are very different then when calculated by
                itself. Remember the whole point of batching is that we should be able to run multiple examples at once
                and get the same values as if we ran them through one at a time
        \end{itemize}
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Attention}
    \begin{pythoncode}
  >>> inv_mask = (1 - mask) * -1e9
  >>> masked_x = (x * mask) + inv_mask
  >>> masked_x
  array([[ 3.2e-01,  1.1e-01,  8.3e-01,  9.0e-01,  4.0e-01],
         [ 5.0e-01,  2.0e-01, -1.0e+00, -1.0e+09, -1.0e+09]])
  >>> softmax(masked_x)
  array([[0.15759942, 0.12774761, 0.26244893, 0.28147862, 0.17072542],
         [0.50917835, 0.3772086 , 0.11361305, 0.        , 0.        ]])
  >>> softmax(x[1:np.newaxis, 0:3])
  >>> array([[0.50917835, 0.3772086 , 0.11361305]])
  >>> softmax(masked_x)sum(axis=1)
  array([1., 1.])
    \end{pythoncode}
    \note{
        \begin{itemize}
            \item We need to hack softmax to force it to output zeros for the padded values while the other values sum
                to one.
            \item We know that the softmax will exponentiate each element and we know that any element to a negative
                number of will approach zero as the exponent grows
            \item With a sufficiently negative number we can force an element to zero
            \item On this first line \texttt{(1 - mask)} will flip a mask of ones and zeros.
            \begin{itemize}
                \item $1 - 1 = 0$ and $1 - 0 = 1$
                \item This will mean that the ones are now where the pads and zero where the valid elements are
            \end{itemize}
            \item Then we multiply in a very negative number so out mask is all zeros and this neg
            \item We multiply by the mask to make sure the padding locations are zero then we add the inverse mask in
            \item the inverse mask has all zeros for the valid elements so they don't change but it has $-1e^9$ for the
                pad so zero + that negative is that negative
            \item Now when we do the softmax we see that out values match and they sum to one in each axis
        \end{itemize}
    }
\end{frame}

        \begin{frame}
            \frametitle{Attention}

            A self attention picture here?

            \note{
                \begin{itemize}
                    \item Masking in attention is also used in transformers to enforce casuality
                    \item The mask stops the attention from looking forward in time
                    \item This mask works the exact same way
                    \item Sometimes you need to combine this causal mask and a length mask to make sure you aren't looking
                        at tokens that don't exist
                    \item Attention is an example where just masking out the value isn't enough, we actually have to
                        manipulate the padding values in a specific way.
                \end{itemize}
            }
        \end{frame}

    \end{subsection} % Attention

    \begin{subsection}{Conditional Random Field}

        \begin{frame}
            \frametitle{CRF}

            This needs a lot more

            CRF are used in sequence tagging to model dependencies in the output

            At each step we calculate the possible scores or transitioning to any state from any state

            Once we hit the end we don't want to overwrite our scores with transistors into the padding

            The CRF gives us a new way to handle batches, we essentially have an if, only update alpha if we are still
            inside the valid lengths. We do an if by calculating the results of both branches (do nothing to the alpha
            and calculate the next step) and use a mask to combine them.

            The Viterbi decoding works the same way

        \end{frame}

    \end{subsection} % CRF

    \begin{subsection}{Max Pooling}

        \begin{frame}
            \frametitle{Max Pooling}
            \begin{align*}
                x &= \begin{bmatrix}
                    1 & 0 & 4 & 3 & 2 & 0 & 0
                \end{bmatrix}\\
                \text{lengths} &= \begin{bmatrix} 5 \end{bmatrix} \\
                \max(x) &= 4
            \end{align*}
            \note{
                \begin{itemize}
                    \item Pop Quiz: Here we have a vector, it has a length of 5 meaning the last two zeros are padding
                    \item Lets say this vector just went through a ReLU $\max(0, x)$ so it will never have a negative
                        number
                    \item Is it save to do a max without thinking about padding?
                    \item \textbf{YES}
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Max Pooling}
            \begin{align*}
                x &= \begin{bmatrix}
                    -3 & -1 & -4 & -3 & -2 & 0 & 0
                \end{bmatrix}\\
                \text{lengths} &= \begin{bmatrix} 5 \end{bmatrix} \\
                \max(x) &= 0 \\
                \max(x[:5]) &= -1
            \end{align*}
            \note{
                \begin{itemize}
                    \item What about a vector that looks like this?
                    \item Max Pooling is often applied along a feature dimension when you have multiple words to select
                        the highest value that feature takes on in any word
                    \item Lets say here we are looking at a feature of a word right after embedding
                    \item Your max pooling it going to output zero for each these!
                    \item Your network will never see a negative value for most examples while training! (most examples
                        are padded)
                    \item Your network will see these values at test time and have no idea what to do!
                    \item How do we fix this?
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Max Pooling}
            \begin{align*}
                x' &= \begin{bmatrix}
                    -3 & -1 & -4 & -3 & -2 & -10000 & -10000
                \end{bmatrix}\\
                \text{lengths} &= \begin{bmatrix} 5 \end{bmatrix} \\
                \max(x') &= -1
            \end{align*}
            \note{
                \begin{itemize}
                    \item We use the same trick we used in attention, by setting the pads so a very negative value we
                        can ensure they will never be picked
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Min Pooling}
            \begin{align*}
                x &= \begin{bmatrix}
                    3 & 1 & 4 & 3 & 2 & 0 & 0
                \end{bmatrix}\\
                x' &= \begin{bmatrix}
                    3 & 1 & 4 & 3 & 2 & 10000 & 10000
                \end{bmatrix}\\
                \text{lengths} &= \begin{bmatrix} 5 \end{bmatrix} \\
                \min(x) &= 0 \\
                \min(x[:5]) &= 1\\
                \min(x') &= 1
            \end{align*}
            \note{
                \begin{itemize}
                    \item This same thing applies in Min pooling but we where the padding can be selected as the minimum
                        even if it isn't. We can solve this similarly but set the padding to a very high value.
                \end{itemize}
            }
        \end{frame}

    \end{subsection} % Max Pooling

    \begin{subsection}{Convolution 1D}

        \begin{frame}
            \frametitle{Convolution 1D}
            GIF of a convolution here?

            \note{
                \begin{itemize}
                    \item Convolutions are used to process basically soft n-grams
                    \item This is commonly used in text classification, the conv will look at multiple words at a time.
                        To create a new representation that is the word contextualized by the words around it
                    \item It is also common in taggers to use a convolution to create a representation of a word based
                        on the characters.
                \end{itemize}
            }

        \end{frame}

        \begin{frame}
            \frametitle{Convolution 1D}

            Diagram of non padded conv creating a shorter vector across multiple slides

            \note{
                \begin{itemize}
                    \item So this shows us how a convolution works, we look at a window, scale the values by a learned
                        weight matrix (not shown here) and sum them to create a new value for this position.
                    \item Unfortunately here we can see that a convolution will actually create a new, shorter sentence
                    \item We need to introduce a new form of padding that I will call conv-padding to distinguish from
                        the padding we have been talking about before
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Convolution 1D}

            Diagram of padded conv creating a sequence of the same length

            \note{
                \begin{itemize}
                    \item Here we can see we are adding a padding of zeros on each side
                    \item There are filtersz // 2 units on each side
                    \item We see here how we now get a vector that is the same length as the input (before padding)
                \end{itemize}
            }
        \end{frame}

        \begin{frame}
            \frametitle{Convolution 1D}

            Diagram of padded conv where there is extra padding

            \note{
                \begin{itemize}
                    \item In this example we are zoommed in on an example that has already been padded, the two zeros at
                        the end
                    \item Now were are going to pad it again because the conv-padding is applied to the whole batch at
                        once
                    \item As we slide the conv we can see that at the end we have spots where the only part of the
                        window is on the valid tokens but it is writing into the output vector.
                    \item This is introducing a new value what could be selected by the max pool during training time
                        that would never happen at test time.
                    \item Does this matter? Yes, when I first noticed this but I trained a tagger on `Conll 2003` and
                        the first thing I trained have slightly different results based on if it was run on a single
                        example of batches so it seems very prevalent.
                    \item "But how do I fix this one Brian?" This is one is pretty easy you just need to zero out the
                        padded values after they go through the conv. If you are following it with maxpool you might
                        want to just jumpr right to masking the pad values with a very negative number
                \end{itemize}
            }
        \end{frame}

    \end{subsection} % Convolution

\end{section}

\begin{section}{Defense}

\begin{frame}[fragile]
    \frametitle{Unit Tests}
    \begin{pythoncode}
def test_batch_stability():
    ex1 = generate_example()
    ext = generate_example()
    res1 = network(ex1, lengths=[len(res1)])
    res2 = network(ex2, lengths=[len(res2)])
    batch, lengths = batch_examples(ex1, ex2)
    res = network(batch, lenghts=lengths)
    batch1, batch2 = extract_results(res, lengths)
    np.testing.assert_allclose(batch1, ex1)
    np.texting.assert_allclose(batch2, ex2)
    \end{pythoncode}

    \href{https://github.com/dpressel/mead-baseline/blob/f98e64afcbab8a267fce5d13a434a981aa564d27/python/tests/test_crf_pytorch.py#L265}{An
    example of these tests in our CRF}

    \note{
        \begin{itemize}
            \item The easiest way to avoid these errors is a unittest, There is a nice recipe for these batch
                instability tests
            \begin{enumerate}
                \item Generate two examples with different lengths
                \item Run each example through the thing you want to test and record results
                \item Batch the two examples together
                \item Run the batch through the thing you want to test
                \item Extract the values that represent each example and compare them to the values from running solo
            \end{enumerate}
        \end{itemize}
    }
\end{frame}

\end{section}

\begin{frame}
    \frametitle{Conclusion}
    \begin{itemize}
        \item Almost nothing is a safe when you are padding.
        \item You should be passing lengths around for most things
        \item Some frameworks help manage this and you should so them when you can.
        \item These bugs are hard to found so you should be writing unittests designed to find them.
    \end{itemize}
    \note{
        \begin{itemize}
            \item as mentioned pytorch loss lets you handle token level losses easy
            \item Most RNN implementations take a length parameter and handle this for you
        \end{itemize}
    }
\end{frame}

\begin{frame}
    \frametitle{Contact}
    \begin{itemize}
        \item \href{}{slides}
        \item \href{}{twitter}
        \item \href{}{github}
    \end{itemize}
    \note{
        Thank you, any questions?
    }
\end{frame}

\end{document}
